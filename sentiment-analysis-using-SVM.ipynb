{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b243456d",
   "metadata": {
    "papermill": {
     "duration": 0.013453,
     "end_time": "2022-01-24T21:39:38.642822",
     "exception": false,
     "start_time": "2022-01-24T21:39:38.629369",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Importation des bibliothèques :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75874625",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-01-24T21:39:38.681077Z",
     "iopub.status.busy": "2022-01-24T21:39:38.677079Z",
     "iopub.status.idle": "2022-01-24T21:39:45.539764Z",
     "shell.execute_reply": "2022-01-24T21:39:45.539014Z",
     "shell.execute_reply.started": "2022-01-24T21:22:20.196230Z"
    },
    "papermill": {
     "duration": 6.884507,
     "end_time": "2022-01-24T21:39:45.539923",
     "exception": false,
     "start_time": "2022-01-24T21:39:38.655416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn import svm\n",
    "import seaborn as sns\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21509c26",
   "metadata": {
    "papermill": {
     "duration": 0.011424,
     "end_time": "2022-01-24T21:39:45.563115",
     "exception": false,
     "start_time": "2022-01-24T21:39:45.551691",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Importation des données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2df27db6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T21:39:45.592409Z",
     "iopub.status.busy": "2022-01-24T21:39:45.591595Z",
     "iopub.status.idle": "2022-01-24T21:39:45.645609Z",
     "shell.execute_reply": "2022-01-24T21:39:45.646306Z",
     "shell.execute_reply.started": "2022-01-24T21:22:40.026769Z"
    },
    "papermill": {
     "duration": 0.071499,
     "end_time": "2022-01-24T21:39:45.646475",
     "exception": false,
     "start_time": "2022-01-24T21:39:45.574976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train= pd.read_csv(\"../input/sentiment-analysis-on-moroccan-arabic-dialect/train.csv\")\n",
    "df_test = pd.read_csv(\"../input/sentiment-analysis-on-moroccan-arabic-dialect/test_stage1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02aafad",
   "metadata": {
    "papermill": {
     "duration": 0.011498,
     "end_time": "2022-01-24T21:39:45.670028",
     "exception": false,
     "start_time": "2022-01-24T21:39:45.658530",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Text Preprocessing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77294ed8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T21:39:45.696841Z",
     "iopub.status.busy": "2022-01-24T21:39:45.695542Z",
     "iopub.status.idle": "2022-01-24T21:39:46.365878Z",
     "shell.execute_reply": "2022-01-24T21:39:46.366313Z",
     "shell.execute_reply.started": "2022-01-24T21:25:42.104406Z"
    },
    "papermill": {
     "duration": 0.684927,
     "end_time": "2022-01-24T21:39:46.366487",
     "exception": false,
     "start_time": "2022-01-24T21:39:45.681560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = stopwords.words('arabic')\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "\n",
    "punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ''' + string.punctuation\n",
    "\n",
    "translator = str.maketrans('', '', punctuations )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b466aa6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T21:39:46.394816Z",
     "iopub.status.busy": "2022-01-24T21:39:46.394107Z",
     "iopub.status.idle": "2022-01-24T21:39:46.410847Z",
     "shell.execute_reply": "2022-01-24T21:39:46.410342Z",
     "shell.execute_reply.started": "2022-01-24T21:26:14.332177Z"
    },
    "papermill": {
     "duration": 0.032112,
     "end_time": "2022-01-24T21:39:46.410995",
     "exception": false,
     "start_time": "2022-01-24T21:39:46.378883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def removeStopWords(text):\n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_sentence_0 = [w for w in word_tokens if not w in stop_words] \n",
    "    text = ' '.join([i for i in filtered_sentence_0])\n",
    "    return text\n",
    "\n",
    "def NormalizeArabic(text):\n",
    "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    return text\n",
    "\n",
    "def arabic_diacritics(text):\n",
    "    arabic_diacritics = re.compile(\"\"\" ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "    return text\n",
    "\n",
    "def removeNumbers(text):\n",
    "    \"\"\" Removes integers \"\"\"\n",
    "    text = ''.join([i for i in text if not i.isdigit()])         \n",
    "    return text\n",
    "\n",
    "def stemming(text):\n",
    "    st = ISRIStemmer()\n",
    "    stemmed_words = []\n",
    "    word_tokens = word_tokenize(text) \n",
    "    for w in word_tokens:\n",
    "        stemmed_words.append(st.stem(w))\n",
    "    stemmed_words = \" \".join(stemmed_words)\n",
    "    return stemmed_words\n",
    "\n",
    "def remove_english_characters(text):\n",
    "        #out = re.sub(r\"[^\\w\\s]\", '', text)\n",
    "        out= re.sub(r'[a-zA-Z]+', '', text)\n",
    "        #out = re.sub(r\"\\n\", '', out)\n",
    "        #out = re.sub(r\"\\s+\", ' ', out)\n",
    "        #out = re.sub(r'[^\\u0600-\\u06FF]', ' ', out)\n",
    "        return out.strip() \n",
    "\n",
    "def removeLetters(text):\n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_sentence = [w for w in word_tokens if len(w)>1] \n",
    "    text = ' '.join([i for i in filtered_sentence])\n",
    "    return text\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations + string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def remove_extra_whitespace(string):\n",
    "    string = re.sub(r'\\s+', ' ', string)\n",
    "    return re.sub(r\"\\s{2,}\", \" \", string).strip()\n",
    "\n",
    "\n",
    "def cleaning (text):\n",
    "  # 1.removing extra spaces\n",
    "    text = re.sub(\"s+\",\" \", text)\n",
    "\n",
    "  # 2.remove repeating char\n",
    "    text= re.sub(r'(.)\\1+', r'\\1', text)  \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ce4d802",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T21:39:46.452806Z",
     "iopub.status.busy": "2022-01-24T21:39:46.452069Z",
     "iopub.status.idle": "2022-01-24T21:39:52.891915Z",
     "shell.execute_reply": "2022-01-24T21:39:52.891396Z",
     "shell.execute_reply.started": "2022-01-24T21:26:35.414112Z"
    },
    "papermill": {
     "duration": 6.468974,
     "end_time": "2022-01-24T21:39:52.892064",
     "exception": false,
     "start_time": "2022-01-24T21:39:46.423090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID                                            comment\n",
      "0   1         لقح نعم لنه فبر , ناس في مصر را كتخلص عليه\n",
      "1   2                          اثب لقح اهم في لحد من وفي\n",
      "2   3  انا لقح جوج رات حمد لله عند نعه ديل حيت درت لق...\n",
      "3   4  كنا تكد من ان جلل ملك جعل لقح مجا لكل غرب وحت ...\n",
      "4   5  شعب انا شبع ثقف خرف مءامره… ندم عندو هني يضر ل...\n",
      "   ID                                            comment  label\n",
      "0   1  انا اوص من هذا نبر لكل توج الي ركز لقح صدقو ام...      1\n",
      "1   2  هناك كثر لا فهم قصد كورو ليس صعب علي شبب لكن ص...      1\n",
      "2   3  حمد لله رقم قبل قرن بدل طقه لول ظهر كورو تحر ه...      1\n",
      "3   4  انا شخص اءد ما فرض سلط من ضرر دلء جوز لقح بهذا...      1\n",
      "4   5  نفس لشء في دين رشد ركز لقح غلق الي غيه اثن اين...      1\n"
     ]
    }
   ],
   "source": [
    "for index, row in df_train.iterrows():\n",
    "    #row['comment'] = removeStopWords(row['comment'])\n",
    "    #row['comment'] = removestpWords(row['comment'])\n",
    "    row['comment'] = removeNumbers(row['comment'])\n",
    "    row['comment'] = remove_english_characters(row['comment'])\n",
    "    row['comment'] = arabic_diacritics(row['comment'])\n",
    "    row['comment'] = NormalizeArabic(row['comment'])\n",
    "    #row['comment'] = remove_punctuations(row['comment'])\n",
    "    #row['comment'] = row['comment'].translate(translator)\n",
    "    #row['comment'] = cleaning(row['comment'])\n",
    "    row['comment'] = stemming(row['comment'])\n",
    "    #row['comment'] = removeLetters(row['comment'])\n",
    "    #row['comment'] = remove_extra_whitespace(row['comment'])\n",
    "    new_df_train = pd.DataFrame({'comment': [row['comment']]}, index=[index])\n",
    "    df_train.update(new_df_train)\n",
    "\n",
    "for index, row in df_test.iterrows():\n",
    "    #row['comment'] = removeStopWords(row['comment'])\n",
    "    #row['comment'] = removestpWords(row['comment'])\n",
    "    row['comment'] = removeNumbers(row['comment'])\n",
    "    row['comment'] = remove_english_characters(row['comment'])\n",
    "    row['comment'] = arabic_diacritics(row['comment'])\n",
    "    row['comment'] = NormalizeArabic(row['comment'])\n",
    "    #row['comment'] = remove_punctuations(row['comment'])\n",
    "    #row['comment'] = row['comment'].translate(translator)\n",
    "    #row['comment'] = cleaning(row['comment'])\n",
    "    row['comment'] = stemming(row['comment'])\n",
    "    #row['comment'] = removeNoise(row['comment'])\n",
    "    #row['comment'] = removeLetters(row['comment'])\n",
    "    #row['comment'] = remove_extra_whitespace(row['comment'])\n",
    "    new_df_test = pd.DataFrame({'comment': [row['comment']]}, index=[index])\n",
    "    df_test.update(new_df_test)\n",
    "    \n",
    "print(df_test.head())\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f4d6a7",
   "metadata": {
    "papermill": {
     "duration": 0.012535,
     "end_time": "2022-01-24T21:39:52.916727",
     "exception": false,
     "start_time": "2022-01-24T21:39:52.904192",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Features extraction (Tfidf) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b151af1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T21:39:53.009071Z",
     "iopub.status.busy": "2022-01-24T21:39:52.971429Z",
     "iopub.status.idle": "2022-01-24T21:39:53.656212Z",
     "shell.execute_reply": "2022-01-24T21:39:53.655588Z",
     "shell.execute_reply.started": "2022-01-24T21:27:27.883486Z"
    },
    "papermill": {
     "duration": 0.726636,
     "end_time": "2022-01-24T21:39:53.656347",
     "exception": false,
     "start_time": "2022-01-24T21:39:52.929711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1920, 52967)\n",
      "(240, 52967)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True , norm='l2', ngram_range=(1, 2))\n",
    "\n",
    "features_train = tfidf.fit_transform(df_train['comment']).toarray()\n",
    "labels_train = df_train['label']\n",
    "\n",
    "print(features_train.shape)\n",
    "\n",
    "features_test = tfidf.transform(df_test['comment']).toarray()\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5978cdfb",
   "metadata": {
    "papermill": {
     "duration": 0.012137,
     "end_time": "2022-01-24T21:39:53.681136",
     "exception": false,
     "start_time": "2022-01-24T21:39:53.668999",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Le classifieur SVM :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3af2eaa7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T21:39:53.712341Z",
     "iopub.status.busy": "2022-01-24T21:39:53.711721Z",
     "iopub.status.idle": "2022-01-24T21:44:26.883071Z",
     "shell.execute_reply": "2022-01-24T21:44:26.883579Z",
     "shell.execute_reply.started": "2022-01-24T21:28:19.253675Z"
    },
    "papermill": {
     "duration": 273.190184,
     "end_time": "2022-01-24T21:44:26.883763",
     "exception": false,
     "start_time": "2022-01-24T21:39:53.693579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, kernel='sigmoid')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = svm.SVC(C=1, gamma='scale' , kernel = 'sigmoid')\n",
    "model.fit(features_train , labels_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e969f935",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T21:44:26.916142Z",
     "iopub.status.busy": "2022-01-24T21:44:26.915471Z",
     "iopub.status.idle": "2022-01-24T21:45:02.359496Z",
     "shell.execute_reply": "2022-01-24T21:45:02.358808Z",
     "shell.execute_reply.started": "2022-01-24T21:36:11.814161Z"
    },
    "papermill": {
     "duration": 35.461893,
     "end_time": "2022-01-24T21:45:02.359671",
     "exception": false,
     "start_time": "2022-01-24T21:44:26.897778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_svc = model.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1063c89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T21:45:02.391310Z",
     "iopub.status.busy": "2022-01-24T21:45:02.390666Z",
     "iopub.status.idle": "2022-01-24T21:45:02.403707Z",
     "shell.execute_reply": "2022-01-24T21:45:02.403124Z",
     "shell.execute_reply.started": "2022-01-24T21:37:43.016348Z"
    },
    "papermill": {
     "duration": 0.030876,
     "end_time": "2022-01-24T21:45:02.403875",
     "exception": false,
     "start_time": "2022-01-24T21:45:02.372999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"../input/sentiment-analysis-on-moroccan-arabic-dialect/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c13b8c6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T21:45:02.434184Z",
     "iopub.status.busy": "2022-01-24T21:45:02.433206Z",
     "iopub.status.idle": "2022-01-24T21:45:02.445586Z",
     "shell.execute_reply": "2022-01-24T21:45:02.446153Z",
     "shell.execute_reply.started": "2022-01-24T21:38:38.131772Z"
    },
    "papermill": {
     "duration": 0.029053,
     "end_time": "2022-01-24T21:45:02.446330",
     "exception": false,
     "start_time": "2022-01-24T21:45:02.417277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission['label']= y_pred_svc \n",
    "submission = submission[['ID','label']]\n",
    "submission.to_csv('submission3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 336.727619,
   "end_time": "2022-01-24T21:45:05.914234",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-24T21:39:29.186615",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
